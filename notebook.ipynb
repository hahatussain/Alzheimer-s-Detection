{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-showtags": false,
    "colab": {
      "name": "5a5d9fff8703d57182c65677157ea222",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x632gtL5fDh"
      },
      "source": [
        "<p style=\"text-align: center\"><img src=\"https://gitlab.aicrowd.com/aicrowd/assets/-/raw/master/challenges/clock-decomposition/notebook-banner.jpg?inline=false\" alt=\"Drawing\" style=\"height: 400px;\"/></p>"
      ],
      "id": "8x632gtL5fDh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": "true",
        "tags": [],
        "id": "KSYifO_n5fDj"
      },
      "source": [
        "\n",
        "# What is the notebook about?\n",
        "\n",
        "The challenge is to use the features extracted from the Clock Drawing Test to build an automated and algorithm to predict whether each participant is one of three phases:\n",
        "\n",
        "1)    Pre-Alzheimer‚Äôs (Early Warning)\n",
        "2)    Post-Alzheimer‚Äôs (Detection)\n",
        "3)    Normal (Not an Alzheimer‚Äôs patient)\n",
        "\n",
        "In machine learning terms: this is a 3-class classification task.\n",
        "\n",
        "# How to use this notebook? üìù\n",
        "\n",
        "<p style=\"text-align: center\"><img src=\"https://gitlab.aicrowd.com/aicrowd/assets/-/raw/master/notebook/aicrowd_notebook_submission_flow.png?inline=false\" alt=\"notebook overview\" style=\"width: 650px;\"/></p>\n",
        "\n",
        "- **Update the config parameters**. You can define the common variables here\n",
        "\n",
        "Variable | Description\n",
        "--- | ---\n",
        "`AICROWD_DATASET_PATH` | Path to the file containing test data (The data will be available at `/ds_shared_drive/` on aridhia workspace). This should be an absolute path.\n",
        "`AICROWD_PREDICTIONS_PATH` | Path to write the output to.\n",
        "`AICROWD_ASSETS_DIR` | In case your notebook needs additional files (like model weights, etc.,), you can add them to a directory and specify the path to the directory here (please specify relative path). The contents of this directory will be sent to AIcrowd for evaluation.\n",
        "`AICROWD_API_KEY` | In order to submit your code to AIcrowd, you need to provide your account's API key. This key is available at https://www.aicrowd.com/participants/me\n",
        "\n",
        "- **Installing packages**. Please use the [Install packages üóÉ](#install-packages-) section to install the packages\n",
        "- **Training your models**. All the code within the [Training phase ‚öôÔ∏è](#training-phase-) section will be skipped during evaluation. **Please make sure to save your model weights in the assets directory and load them in the predictions phase section** "
      ],
      "id": "KSYifO_n5fDj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "VLon-mvy5fDl"
      },
      "source": [
        "# Setup AIcrowd Utilities üõ†\n",
        "\n",
        "We use this to bundle the files for submission and create a submission on AIcrowd. Do not edit this block."
      ],
      "id": "VLon-mvy5fDl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-90Nbfpv5fDl"
      },
      "source": [
        "!pip install -q -U aicrowd-cli --use-feature=2020-resolver"
      ],
      "id": "-90Nbfpv5fDl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcnMrW6J5fDm",
        "outputId": "242634ce-0a08-4914-d91b-61e6d6f9407b"
      },
      "source": [
        "%load_ext aicrowd.magic"
      ],
      "id": "ZcnMrW6J5fDm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The aicrowd.magic extension is already loaded. To reload it, use:\n",
            "  %reload_ext aicrowd.magic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhRXJBP-5fDn"
      },
      "source": [
        "!pip install -q numpy pandas scikit-learn\n",
        "!pip install -q -U fastcore fastai"
      ],
      "id": "fhRXJBP-5fDn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "v114JIfm5fDn"
      },
      "source": [
        "# AIcrowd Runtime Configuration üß∑\n",
        "\n",
        "Define configuration parameters. Please include any files needed for the notebook to run under `ASSETS_DIR`. We will copy the contents of this directory to your final submission file üôÇ\n",
        "\n",
        "The dataset is available under `/ds_shared_drive` on the workspace."
      ],
      "id": "v114JIfm5fDn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qslCvVe5fDp"
      },
      "source": [
        "import os\n",
        "\n",
        "# Please use the absolute for the location of the dataset.\n",
        "# Or you can use relative path with `os.getcwd() + \"test_data/validation.csv\"`\n",
        "AICROWD_DATASET_PATH = os.getenv(\"DATASET_PATH\", \"/ds_shared_drive/validation.csv\")\n",
        "AICROWD_PREDICTIONS_PATH = os.getenv(\"PREDICTIONS_PATH\", \"predictions.csv\")\n",
        "AICROWD_ASSETS_DIR = \"assets\"\n"
      ],
      "id": "8qslCvVe5fDp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "uj1JcmlI5fDp"
      },
      "source": [
        "# Install packages üóÉ\n",
        "\n",
        "Please add all pacakage installations in this section"
      ],
      "id": "uj1JcmlI5fDp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDKVjNM35fDp",
        "outputId": "023bd3b4-9e56-40c5-a67a-60cfa723da1b"
      },
      "source": [
        "!pip install numpy pandas"
      ],
      "id": "FDKVjNM35fDp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (1.19.2)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (1.1.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6etv88IN5fDq",
        "outputId": "aafb9d45-da6c-431e-c403-d131637c76e6"
      },
      "source": [
        "!pip install -q numpy pandas scikit-learn\n",
        "!pip install -q -U fastcore fastai\n",
        "!pip install tensorflow_decision_forests\n",
        "!pip install wurlitzer"
      ],
      "id": "6etv88IN5fDq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_decision_forests in /opt/anaconda3/lib/python3.8/site-packages (0.1.5)\n",
            "Requirement already satisfied: tensorflow~=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow_decision_forests) (2.5.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow_decision_forests) (1.19.2)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow_decision_forests) (1.1.3)\n",
            "Requirement already satisfied: wheel in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow_decision_forests) (0.35.1)\n",
            "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow_decision_forests) (0.12.0)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow_decision_forests) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (3.15.8)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (2.5.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (1.34.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (1.12.1)\n",
            "Requirement already satisfied: gast==0.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (0.4.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (2.5.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow~=2.5->tensorflow_decision_forests) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas->tensorflow_decision_forests) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/lib/python3.8/site-packages (from pandas->tensorflow_decision_forests) (2020.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (0.6.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (1.30.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (50.3.1.post20201107)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (4.2.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (1.25.11)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (2020.6.20)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /opt/anaconda3/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5->tensorflow_decision_forests) (0.4.8)\n",
            "Requirement already satisfied: wurlitzer in /opt/anaconda3/lib/python3.8/site-packages (2.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "09qROraD5fDq"
      },
      "source": [
        "# Define preprocessing code üíª\n",
        "\n",
        "The code that is common between the training and the prediction sections should be defined here. During evaluation, we completely skip the training section. Please make sure to add any common logic between the training and prediction sections here."
      ],
      "id": "09qROraD5fDq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuY00T1D5fDq"
      },
      "source": [
        "### Import common packages\n",
        "\n",
        "Please import packages that are common for training and prediction phases here."
      ],
      "id": "uuY00T1D5fDq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVWIPife5fDr"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from fastai.tabular.all import *\n",
        "import sklearn as sk\n",
        "from sklearn.metrics import f1_score, log_loss\n",
        "import tensorflow as tf\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import pickle\n",
        "from sklearn import preprocessing\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, log_loss\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)"
      ],
      "id": "pVWIPife5fDr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "S24QBEOY5fDr"
      },
      "source": [
        "# Training phase ‚öôÔ∏è\n",
        "\n",
        "You can define your training code here. This sections will be skipped during evaluation."
      ],
      "id": "S24QBEOY5fDr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaUZenRi5fDs"
      },
      "source": [
        "## Load training data"
      ],
      "id": "gaUZenRi5fDs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5uttnuI5fDs"
      },
      "source": [
        "df = pd.read_csv(os.getenv(\"DATASET_PATH\", \"/ds_shared_drive/train.csv\"))\n",
        "df_val = pd.read_csv(os.getenv(\"DATASET_PATH\", \"/ds_shared_drive/validation.csv\"))\n",
        "\n",
        "# Still not done, here we smash the previous dataset with the ground-truth\n",
        "df_val = pd.merge(df_val, pd.read_csv(os.getenv(\"DATASET_PATH\", \"/ds_shared_drive/validation_ground_truth.csv\")), how='left', on='row_id')\n",
        "\n",
        "# col =[]\n",
        "# for i,name in enumerate(df.columns):\n",
        "#     if i>0:\n",
        "#         col.append(name)\n",
        "\n",
        "# len(col)\n",
        "# df.dropna(axis='rows',subset=col,inplace=True)\n",
        "# df.reset_index(drop=True)\n",
        "# load your data"
      ],
      "id": "Z5uttnuI5fDs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKIxEgpX5fDs"
      },
      "source": [
        "df['intersection_pos_rel_centre'].fillna('N', inplace=True)\n",
        "df_val['intersection_pos_rel_centre'].fillna('N', inplace=True)\n",
        "\n",
        "df_dummies = pd.get_dummies(df['intersection_pos_rel_centre'], columns='intersection_pos_rel_centre',\n",
        "                          dummy_na=True).add_prefix('c_')\n",
        "\n",
        "df_val_dummies = pd.get_dummies(df_val['intersection_pos_rel_centre'], columns='intersection_pos_rel_centre',\n",
        "                          dummy_na=True).add_prefix('c_')\n",
        "\n",
        "\n",
        "#and then we drop the original ones from the datasets\n",
        "df = df.drop('intersection_pos_rel_centre', axis=1)\n",
        "df_val = df_val.drop('intersection_pos_rel_centre', axis=1)\n",
        "\n",
        "#our new sets are the concatenation of the last ones\n",
        "df = pd.concat([df, df_dummies], axis=1)\n",
        "df_val = pd.concat([df_val, df_val_dummies], axis=1)"
      ],
      "id": "wKIxEgpX5fDs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfTjZv8g5fDs"
      },
      "source": [
        "X = df.drop(['row_id', 'diagnosis'], axis=1)\n",
        "# we save the diagnosis as our target \n",
        "y = df['diagnosis']\n",
        "\n",
        "# And we create our validation vectors\n",
        "X_val = df_val.drop(['row_id', 'diagnosis'], axis=1)\n",
        "y_val = df_val['diagnosis']"
      ],
      "id": "VfTjZv8g5fDs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p8kprqn5fDt"
      },
      "source": [
        "imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=999)\n",
        "imputer.fit(pd.concat([X,X_val]))\n",
        "SimpleImputer()\n",
        "X_imputed = imputer.transform(X)\n",
        "X_val = imputer.transform(X_val)\n"
      ],
      "id": "_p8kprqn5fDt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTWptQno5fDt"
      },
      "source": [
        "## Train your model"
      ],
      "id": "gTWptQno5fDt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30asRxcc5fDt",
        "outputId": "62200501-828f-4db1-c2ae-05788dd709aa"
      },
      "source": [
        "# model.fit(train_data)\n",
        "model = RandomForestClassifier(n_estimators=1000)\n",
        "model.fit(X_imputed, y)\n",
        "\n",
        "log_loss_value = log_loss(y_val, model.predict_proba(X_val))\n",
        "f1_value = f1_score(y_val, model.predict(X_val), average='macro')\n",
        "\n",
        "print(\"log_loss_value over validation = {}\\nf1_value over validation = {}\".format(log_loss_value, f1_value))"
      ],
      "id": "30asRxcc5fDt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_loss_value over validation = 0.715918873159399\n",
            "f1_value over validation = 0.35465225811799633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jifFlHX5fDt"
      },
      "source": [
        "# some custom code block"
      ],
      "id": "8jifFlHX5fDt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V02LK1iL5fDu"
      },
      "source": [
        "## Save your trained model"
      ],
      "id": "V02LK1iL5fDu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnW9F96S5fDu"
      },
      "source": [
        "# model.save()\n",
        "filename = f'{AICROWD_ASSETS_DIR}/haha'\n",
        "torch.save(model, filename)\n"
      ],
      "id": "XnW9F96S5fDu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "p7lwX8Lf5fDu"
      },
      "source": [
        "# Prediction phase üîé\n",
        "\n",
        "Please make sure to save the weights from the training section in your assets directory and load them in this section"
      ],
      "id": "p7lwX8Lf5fDu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbgKcOga5fDu"
      },
      "source": [
        "# model = load_model_from_assets_dir(AIcrowdConfig.ASSETS_DIR)\n",
        "filename = f'{AICROWD_ASSETS_DIR}/haha'\n",
        "model = torch.load(filename)"
      ],
      "id": "TbgKcOga5fDu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1pwhHJw5fDu"
      },
      "source": [
        "## Load test data"
      ],
      "id": "q1pwhHJw5fDu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDa7-SgA5fDu"
      },
      "source": [
        "test_data = pd.read_csv(AICROWD_DATASET_PATH)\n",
        "\n",
        "test_data['intersection_pos_rel_centre'].fillna('N', inplace=True)\n",
        "\n",
        "test_dummies = pd.get_dummies(test_data['intersection_pos_rel_centre'], columns='intersection_pos_rel_centre',\n",
        "                          dummy_na=True).add_prefix('c_')\n",
        "\n",
        "test = test_data.drop(['row_id','intersection_pos_rel_centre'], axis=1)\n",
        "\n",
        "test = pd.concat([test, test_dummies], axis=1)\n",
        "\n",
        "imputer_test = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=999)\n",
        "imputer_test.fit(test)\n",
        "SimpleImputer()\n",
        "test = imputer_test.transform(test)"
      ],
      "id": "rDa7-SgA5fDu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELe9Wlz75fDv"
      },
      "source": [
        "## Generate predictions"
      ],
      "id": "ELe9Wlz75fDv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMMjy7ay5fDv"
      },
      "source": [
        "predict = model.predict_proba(test)\n",
        "\n",
        "predictions = {\n",
        "    \"row_id\": test_data[\"row_id\"].values,\n",
        "    \"normal_diagnosis_probability\": predict[:,0],\n",
        "    \"post_alzheimer_diagnosis_probability\": predict[:,1],\n",
        "    \"pre_alzheimer_diagnosis_probability\": predict[:,2],\n",
        "}\n",
        "\n",
        "predictions_df = pd.DataFrame.from_dict(predictions)"
      ],
      "id": "jMMjy7ay5fDv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ej_1pBC5fDv",
        "outputId": "18d3ff27-737d-462f-c4c6-fcc3cf04f607"
      },
      "source": [
        "predictions_df"
      ],
      "id": "5Ej_1pBC5fDv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>normal_diagnosis_probability</th>\n",
              "      <th>post_alzheimer_diagnosis_probability</th>\n",
              "      <th>pre_alzheimer_diagnosis_probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LA9JQ1JZMJ9D2MBZV</td>\n",
              "      <td>0.867</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PSSRCWAPTAG72A1NT</td>\n",
              "      <td>0.853</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GCTODIZJB42VCBZRZ</td>\n",
              "      <td>0.997</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7YMVQGV1CDB1WZFNE</td>\n",
              "      <td>0.769</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PHEQC6DV3LTFJYIJU</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.279</td>\n",
              "      <td>0.021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>SDM0DQJ0Z1L72FBQG</td>\n",
              "      <td>0.999</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>3A7NVWPQEHUGYJUH0</td>\n",
              "      <td>0.796</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>S36ZWGFUK77RAOSV1</td>\n",
              "      <td>0.949</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>LFYFH8E7EP75VLWNW</td>\n",
              "      <td>0.921</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>74XBFYQCZCNVQ3KOA</td>\n",
              "      <td>0.826</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.079</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>362 rows √ó 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                row_id  normal_diagnosis_probability  \\\n",
              "0    LA9JQ1JZMJ9D2MBZV                         0.867   \n",
              "1    PSSRCWAPTAG72A1NT                         0.853   \n",
              "2    GCTODIZJB42VCBZRZ                         0.997   \n",
              "3    7YMVQGV1CDB1WZFNE                         0.769   \n",
              "4    PHEQC6DV3LTFJYIJU                         0.700   \n",
              "..                 ...                           ...   \n",
              "357  SDM0DQJ0Z1L72FBQG                         0.999   \n",
              "358  3A7NVWPQEHUGYJUH0                         0.796   \n",
              "359  S36ZWGFUK77RAOSV1                         0.949   \n",
              "360  LFYFH8E7EP75VLWNW                         0.921   \n",
              "361  74XBFYQCZCNVQ3KOA                         0.826   \n",
              "\n",
              "     post_alzheimer_diagnosis_probability  pre_alzheimer_diagnosis_probability  \n",
              "0                                   0.093                                0.040  \n",
              "1                                   0.084                                0.063  \n",
              "2                                   0.003                                0.000  \n",
              "3                                   0.205                                0.026  \n",
              "4                                   0.279                                0.021  \n",
              "..                                    ...                                  ...  \n",
              "357                                 0.001                                0.000  \n",
              "358                                 0.167                                0.037  \n",
              "359                                 0.037                                0.014  \n",
              "360                                 0.062                                0.017  \n",
              "361                                 0.095                                0.079  \n",
              "\n",
              "[362 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx2XdIlC5fDv"
      },
      "source": [
        "## Save predictions üì®"
      ],
      "id": "zx2XdIlC5fDv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WOMoIZP5fDv"
      },
      "source": [
        "predictions_df.to_csv(AICROWD_PREDICTIONS_PATH, index=False)"
      ],
      "id": "9WOMoIZP5fDv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ugEBsKhA5fDw"
      },
      "source": [
        "# Submit to AIcrowd üöÄ\n",
        "\n",
        "**NOTE: PLEASE SAVE THE NOTEBOOK BEFORE SUBMITTING IT (Ctrl + S)**"
      ],
      "id": "ugEBsKhA5fDw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HclbUpf85fDw",
        "outputId": "e9915d05-dfd0-49ca-fce0-c7f796e78e95"
      },
      "source": [
        "!DATASET_PATH=$AICROWD_DATASET_PATH \\\n",
        "aicrowd notebook submit \\\n",
        "    --assets-dir $AICROWD_ASSETS_DIR \\\n",
        "    --challenge addi-alzheimers-detection-challenge"
      ],
      "id": "HclbUpf85fDw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32mAPI Key valid\u001b[0m\n",
            "\u001b[32mSaved API Key successfully!\u001b[0m\n",
            "Using notebook: /home/desktop0/haha.ipynb for submission...\n",
            "Removing existing files from submission directory...\n",
            "Scrubbing API keys from the notebook...\n",
            "Collecting notebook...\n",
            "Validating the submission...\n",
            "Executing install.ipynb...\n",
            "[NbConvertApp] Converting notebook /home/desktop0/submission/install.ipynb to notebook\n",
            "[NbConvertApp] Executing notebook with kernel: python\n",
            "[NbConvertApp] Writing 10217 bytes to /home/desktop0/submission/install.nbconvert.ipynb\n",
            "Executing predict.ipynb...\n",
            "[NbConvertApp] Converting notebook /home/desktop0/submission/predict.ipynb to notebook\n",
            "[NbConvertApp] Executing notebook with kernel: python\n",
            "2021-06-07 18:01:06.588982: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2021-06-07 18:01:06.589025: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/bin/jupyter-nbconvert\", line 11, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/jupyter_core/application.py\", line 270, in launch_instance\n",
            "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n",
            "    app.start()\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/nbconvertapp.py\", line 350, in start\n",
            "    self.convert_notebooks()\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/nbconvertapp.py\", line 524, in convert_notebooks\n",
            "    self.convert_single_notebook(notebook_filename)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/nbconvertapp.py\", line 489, in convert_single_notebook\n",
            "    output, resources = self.export_single_notebook(notebook_filename, resources, input_buffer=input_buffer)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/nbconvertapp.py\", line 418, in export_single_notebook\n",
            "    output, resources = self.exporter.from_filename(notebook_filename, resources=resources)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/exporters/exporter.py\", line 181, in from_filename\n",
            "    return self.from_file(f, resources=resources, **kw)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/exporters/exporter.py\", line 199, in from_file\n",
            "    return self.from_notebook_node(nbformat.read(file_stream, as_version=4), resources=resources, **kw)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/exporters/notebook.py\", line 32, in from_notebook_node\n",
            "    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/exporters/exporter.py\", line 143, in from_notebook_node\n",
            "    nb_copy, resources = self._preprocess(nb_copy, resources)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/exporters/exporter.py\", line 318, in _preprocess\n",
            "    nbc, resc = preprocessor(nbc, resc)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/preprocessors/base.py\", line 47, in __call__\n",
            "    return self.preprocess(nb, resources)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/preprocessors/execute.py\", line 79, in preprocess\n",
            "    self.execute()\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbclient/util.py\", line 74, in wrapped\n",
            "    return just_run(coro(*args, **kwargs))\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbclient/util.py\", line 53, in just_run\n",
            "    return loop.run_until_complete(coro)\n",
            "  File \"/opt/anaconda3/lib/python3.8/asyncio/base_events.py\", line 616, in run_until_complete\n",
            "    return future.result()\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbclient/client.py\", line 540, in async_execute\n",
            "    await self.async_execute_cell(\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/preprocessors/execute.py\", line 123, in async_execute_cell\n",
            "    cell, resources = self.preprocess_cell(cell, self.resources, cell_index)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbconvert/preprocessors/execute.py\", line 146, in preprocess_cell\n",
            "    cell = run_sync(NotebookClient.async_execute_cell)(self, cell, index, store_history=self.store_history)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbclient/util.py\", line 74, in wrapped\n",
            "    return just_run(coro(*args, **kwargs))\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbclient/util.py\", line 53, in just_run\n",
            "    return loop.run_until_complete(coro)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nest_asyncio.py\", line 96, in run_until_complete\n",
            "    return f.result()\n",
            "  File \"/opt/anaconda3/lib/python3.8/asyncio/futures.py\", line 178, in result\n",
            "    raise self._exception\n",
            "  File \"/opt/anaconda3/lib/python3.8/asyncio/tasks.py\", line 280, in __step\n",
            "    result = coro.send(None)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbclient/client.py\", line 832, in async_execute_cell\n",
            "    self._check_raise_for_error(cell, exec_reply)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/nbclient/client.py\", line 740, in _check_raise_for_error\n",
            "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply['content'])\n",
            "nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n",
            "------------------\n",
            "# model = load_model_from_assets_dir(AIcrowdConfig.ASSETS_DIR)\n",
            "filename = f'{AICROWD_ASSETS_DIR}/haha'\n",
            "model = torch.load(filename)\n",
            "------------------\n",
            "\n",
            "\n",
            "NameError: name 'torch' is not defined\n",
            "\n",
            "\u001b[31mLocal Evaluation Error Error: predict.ipynb failed to execute\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqQnnzZN5fDw"
      },
      "source": [
        ""
      ],
      "id": "YqQnnzZN5fDw",
      "execution_count": null,
      "outputs": []
    }
  ]
}